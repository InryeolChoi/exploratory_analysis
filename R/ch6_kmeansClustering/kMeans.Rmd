---
title: "kMeans"
author: "inryeol"
date: "2024-08-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
```
# 개요 : K-Means

- 그루핑을 할 때마다 재계산. 따라서 hierarchical와 속도에서 차이가 난다.
- **먼저 클러스터의 갯수 k를 정하고, 거리를 계산한다.**
    - hierarchical과의 가장 큰 차이점.
- 자료가 있다면, 계산을 할 때마다 공간을 나눈다.

## 알고리즘과 결과

1. k를 정한다. 최소한 2보다는 커야 한다.
2. 각 클러스터의 centroid를 랜덤으로 찍고, 거리 계산. 
3. centroid에서 가장 가까운 점을 고른다. 이를 기준으로 클러스터링.
4. 묶인 파트를 기준으로 또 centroid 찍기.
5. 반복 → 안 묶일때까지!

**묶이지 않는 기준 = 점들이 움직이지 않을 경우.** 
결과 1 : 최적의 centriod.
결과 2 : 각 점별 할당된 그룹을 알려줌 (파티셔닝 결과)

## 근접성 파악

- 클러스터링, 다변량 scaling, 비선형 차원축소 기법에서 중요
    - 각각의 변수를 하나씩 표준화하는 것 ⇒ marginally

min=0, max=1 ⇒ 각 변수들의 차이 고려

$$
{x_i}^{*} = 
{{x_{ij}} - min(\underline{x})
\over
max(\underline{x})- min(\underline{x})}
$$

mean = 0, std = 1 ⇒ z 표준화

$$
{x_{ij}}^{*} = {x_{ij} - \overline{x_j}\over s_{x_j}}

$$

이산형 데이터를 클러스터링? 좋은 데이터라고 보기는 어렵다.

- 임의로 숫자를 할당
- 단, 0과 1로 코딩되는 경우(성별) 클러스터링 가능

이산형 데이터를 클러스터링? 좋은 데이터라고 보기는 어렵다.

- 임의로 숫자를 할당
- 단, 0과 1로 코딩되는 경우(성별) 클러스터링 가능

## 유사도 측정 기법

1. 코사인 유사도 (연속형 변수)
2. 상관관계 유사도 (연속형 변수)
3. jaccard 유사도
4. Ochiai 측도
5. Simple matching 계수

## 비유사도 측정 기법
1. 유클리드 거리
2. 마할라노비스 거리
3. 민코프스키 거리

# 예시
## 데이터 준비
데이터를 준비한다. 시드값은 1234. 정규분포에서 뽑기.  
x ⇒ 12개. 평균은 1,2,3을 각각 4번씩. 표준편차는 0.2  
y ⇒ 12개. 평균은 1,2,1을 각각 4번씩. 표준편차는 0.2  
```{r}
# 데이터 준비 & plot 그리기 
rm(list=ls())
set.seed(1234)
x = rnorm(12, mean=rep(1:3, each=4), sd=0.2)
y = rnorm(12, mean = rep(c(1, 2, 1), each=4),
          sd=0.2)
plot(x, y, col="blue", cex=2, pch=19)
text(x+0.05, y+0.05, as.character(1:12))
```

## 파트 1 : kmeans() 쓰기
`kmeans()`는 유클리드 거리를 기준으로 한다.
1. 라이브러리 `stats` 사용
2. 데이터프레임 생성
3. `kmeans()` 를 이용해 클러스터링을 한다. `centers` = 3
    
**힌트 : 반복 횟수(iter.max)를 100회로 지정.** 
**(안 하면 클러스터링이 1번으로 끝)**
    
4. 변수 `kmeans_Obj` 에 값을 할당한다.

```{r}
# kmeans() 써보기
library(stats)
dataFrame = data.frame(x, y)
kmeans_obj = kmeans(dataFrame, centers=3, iter.max=100)
```
변수 kmeans_obj를 보고, 클러스터 이름을 보자.
```{r}
names(kmeans_obj)
kmeans_obj$cluster
```
- `cluster` : 몇 번째 데이터가 어디 그룹인지 나타낸다.
- `centers` : 중심점을 보여준다.
- `totss` : 그룹 내 거리 제곱의 합 (sum of squares)
- `withinss` : Vector of within-cluster sum of squares, one component per cluster.
- `tot.withinss` : withiness의 합
- `betweenss` : between-cluster sum of squares
- `size` : The number of points in each cluster.
- `iter` : 반복횟수
- `ifault` : indicator of a possible algorithm problem


## 파트 2 : 그림 그려보기
**솔루션을 그림으로 그려보자.**

```{r}
# 최종결과
plot(x, y, col=kmeans_obj$cluster, cex=2, pch=19)
points(kmeans_obj$centers, pch=3, cex=2)
```
그러나 이러한 그림은 변수가 2~3개 이상인 경우 그리기 힘들다.
그러한 경우에는 히트맵을 써야 한다.

## 파트 3 : 히트맵 그리기
`heatmap.2()` 대신 `image()`를 이용해서 덴드로그램 그리고, 센터 찾기
`heatmap.2()` 로 k-means 클러스터링을 그리면 너무 복잡하다.

```{r}
# 히트맵 - 데이터 준비
set.seed(1234)
datamat = as.matrix(dataFrame)[sample(1:12),]
kmeans_obj = kmeans(datamat, centers=3)

# 히트맵 만들기 1
library(gplots)
par(mfrow=c(1,2))
```
그림을 그리고 난 뒤, 센터도 한번 확인해보자.

**`heatmap.2()` 로 그린 히트맵**
```{r}
# density.info = none
heatmap.2(datamat, trace='none', notecol = 'black', density.info = 'none', dendrogram="none")
```

# 클러스터 평가하기

- 몇 개의 클러스터가 적절한가?
- 클러스터링의 목적: 다른 그룹과 잘 분리, 같은 그룹과 잘 묶임.

## 실루엣 통계량

- number of groups를 추정하는 방법

```{r}
# 클러스터 평가하기
library(cluster)

data.dist = dist(datamat)
data.km2 = kmeans(datamat, centers=2)
data.km3 = kmeans(datamat, centers=3)
data.km4 = kmeans(datamat, centers=4)

# silhouette() 이용하기
data.km2.sil = silhouette(data.km2$cluster, data.dist)
data.km3.sil = silhouette(data.km3$cluster, data.dist)
data.km4.sil = silhouette(data.km4$cluster, data.dist)

# 그리기
par(mfrow=c(1, 3))
plot(data.km2.sil)
plot(data.km3.sil)
plot(data.km4.sil)
```


```{r}
# 히트맵 만들기 2
par(mfrow=c(1,2))
image(t(datamat)[, nrow(datamat):1], yaxt="n", main="original data")
image(t(datamat)[, order(kmeans_obj$cluster)], yaxt="n", main="clustered data")

# 사각형 씌우기
plot(hcluster)
rect.hclust(hcluster, k=2)
plot(hcluster)
rect.hclust(hcluster, k=4)
```
